{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark[sql] in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[sql]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[sql]) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[sql]) (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[sql]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (5.22.0)\n",
      "Requirement already satisfied: pyspark[pandas_on_spark] in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (2.0.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from plotly) (8.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[pandas_on_spark]) (1.16.0)\n",
      "Requirement already satisfied: pyspark[connect] in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (16.1.0)\n",
      "Requirement already satisfied: grpcio>=1.56.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (1.64.1)\n",
      "Requirement already satisfied: grpcio-status>=1.56.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (1.64.1)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.56.4 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (1.63.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pyspark[connect]) (2.0.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from googleapis-common-protos>=1.56.4->pyspark[connect]) (5.27.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[connect]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[connect]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[connect]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect]) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isals\\onedrive\\documents\\github\\pyspark-playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# nstall and import Dependencies\n",
    "%pip install pyspark\n",
    "%pip install pyspark[sql]\n",
    "%pip install pyspark[pandas_on_spark] plotly\n",
    "%pip install pyspark[connect]\n",
    "%pip install pandas\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Initiating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://FAISAL-SAMUDRA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e23f9ae9f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practice').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show pySpark data including header\n",
      "\n",
      "Type of df_pyspark: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "Print Schema:\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data from csv file\n",
    "df_pyspark=spark.read.csv('sample.csv')\n",
    "\n",
    "# Load and show data from csv file including header\n",
    "print(\"Show pySpark data including header\")\n",
    "df_pyspark=spark.read.option('header','true').csv('sample.csv')\n",
    "print(f\"\\nType of df_pyspark: {type(df_pyspark)}\")\n",
    "\n",
    "# Print schema\n",
    "print(\"\\nPrint Schema:\")\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Basics of pySpark\n",
    "- PySpark Dataframe\n",
    "- Reading dataset\n",
    "- Check the datatypes of column (Schema)\n",
    "- Selecting columns and indexing\n",
    "- Check describe option similar to pandas\n",
    "- Adding columns\n",
    "- Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|Faisal| 27|         5|\n",
      "| Ifaaf| 27|         4|\n",
      "|Yunkai| 26|         5|\n",
      "+------+---+----------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n",
      "\n",
      "Get column names:\n",
      "+-------+------+------------------+------------------+\n",
      "|summary|  Name|               Age|        Experience|\n",
      "+-------+------+------------------+------------------+\n",
      "|  count|     3|                 3|                 3|\n",
      "|   mean|  NULL|26.666666666666668| 4.666666666666667|\n",
      "| stddev|  NULL|0.5773502691896258|0.5773502691896258|\n",
      "|    min|Faisal|                26|                 4|\n",
      "|    max|Yunkai|                27|                 5|\n",
      "+-------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##$ Getting attributes\n",
    "\n",
    "# Read the data from csv file with header\n",
    "df_pyspark=spark.read.option('header','true').csv('sample.csv', inferSchema=True)\n",
    "df_pyspark.show()\n",
    "\n",
    "# Check the schema\n",
    "df_pyspark.printSchema()\n",
    "\n",
    "# Get column names\n",
    "print(\"\\nGet column names:\")\n",
    "df_pyspark.columns\n",
    "\n",
    "df_pyspark.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get first 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Name='Faisal', Age=27, Experience=5),\n",
       " Row(Name='Ifaaf', Age=27, Experience=4),\n",
       " Row(Name='Yunkai', Age=26, Experience=5)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get first 3 rows\n",
    "print(\"\\nGet first 3 rows:\")\n",
    "df_pyspark.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Faisal|\n",
      "| Ifaaf|\n",
      "|Yunkai|\n",
      "+------+\n",
      "\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Faisal| 27|\n",
      "| Ifaaf| 27|\n",
      "|Yunkai| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Selecting Columns\n",
    "\n",
    "# Select specific columns\n",
    "df_pyspark.select('Name').show()\n",
    "\n",
    "# Select multiple columns\n",
    "df_pyspark.select(['Name','Age']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------------------+\n",
      "|  Name|Age|Experience|Experience + 2 year|\n",
      "+------+---+----------+-------------------+\n",
      "|Faisal| 27|         5|                  7|\n",
      "| Ifaaf| 27|         4|                  6|\n",
      "|Yunkai| 26|         5|                  7|\n",
      "+------+---+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding columns in DataFrame\n",
    "df_pyspark_full = df_pyspark.withColumn('Experience + 2 year', df_pyspark['Experience']+2)\n",
    "df_pyspark_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|Faisal| 27|         5|\n",
      "| Ifaaf| 27|         4|\n",
      "|Yunkai| 26|         5|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop columns in DataFrame\n",
    "df_pyspark_full = df_pyspark.drop('Experience + 2 year')\n",
    "df_pyspark_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------------+\n",
      "|  Name|Age|Experience (yrs)|\n",
      "+------+---+----------------+\n",
      "|Faisal| 27|               5|\n",
      "| Ifaaf| 27|               4|\n",
      "|Yunkai| 26|               5|\n",
      "+------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename columns in DataFrame\n",
    "df_pyspark_full = df_pyspark.withColumnRenamed('Experience','Experience (yrs)')\n",
    "df_pyspark_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
